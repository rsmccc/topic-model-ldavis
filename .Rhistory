stack(wind)
swind <- stack(wind)
names(swind) <- c("Season", "values")
names(swind) <- c("values", "Season")
?stack()
swind$cos <- cos(swind$values)
swind$sin <- sin(swind$values)
plot(x = swind$cos, y = swind$sin, col = swind$Season, main = "Wind Direction in different seasons", pch = 16, ylab = "sin(Direction)", xlab = "cos(Direction)")
legend(-0.5, 0.7, as.vector(levels(swind$Season)), pch=16, col = c(1:4))
titanic <- read.csv("http://maitra.public.iastate.edu/stat579/datasets/titanic.txt")
sum(is.na(titanic$Age)) ## 557 NA's in age
class.gender <- table(titanic$PClass, titanic$Sex)
class.gender ### Far more men on the ship than women
sum(titanic[titanic$Sex == "male"]) / length(titanic$Sex)
sum(titanic$Sex[titanic$Sex == "male"]) / length(titanic$Sex)
count(titanic$Sex[titanic$Sex == "male"]) / length(titanic$Sex)
length(titanic$Sex[titanic$Sex == "male"]) / length(titanic$Sex)
length(titanic$Sex[titanic$Sex == "male"])
length(titanic$Sex)
length(titanic$Sex[titanic$Sex == "male"]) / length(titanic$Sex)
class.gender.survive <- table(titanic$PClass, titanic$Sex, titanic$Survived)
class.gender.survive
length(titanic$Survived[titanic$Survived == "male"]) / length(titanic$Survived) ## 64.8% male
length(titanic$Survived[titanic$Survived == 1]) / length(titanic$Survived) ## 64.8% male
length(titanic$Survived[titanic$Survived == 1 & titanic$Sex == "female"]) / length(titanic$Survived[titanic$Sex == "female"]) ## 34.3% survived
length(titanic$Survived[titanic$Survived == 1 & titanic$Sex == "male"]) / length(titanic$Survived[titanic$Sex == "male"])
length(titanic)
length(titanic$Name)
length(titanic$Age)
class.gender.survive.rate <- (class.gender.survive[,,2] / (class.gender)) * 100
class.gender.survive.rate
m.age.surv <- titanic[titanic$Survived == 1 & titanic$Sex == "male",]
m.age.surv
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male",])
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"])
age.surv
titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T)
age.surv
age.surv <- as.data.frame(age.surv, col.names = "men.surv")
names(age.surv) <- "male.surv"
age.surv
mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T)
age.surv <- as.data.frame(mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T))
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men.Surv"
age.surv$Men.Dead <- mean(titanic$Age[titanic$Survived == 0 & titanic$Sex == "male"], na.rm = T)
age.surv$Women.Surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "female"], na.rm = T)
age.surv$Women.Dead <- mean(titanic$Age[titanic$Survived == 0 & titanic$Sex == "female"], na.rm = T)
age.surv[0]
age.surv[1]
age.surv
age.surv[2]
detach("package:reshape", unload=TRUE)
?reshape()
age.surv$Labels <- c("Mean", "SE")
age.surv$Men.Surv[2]
age.surv$Men.Surv[1]
age.surv[2]
age.surv$Men.Surv[2] <- 0
nrow(age.surv)
nrow(age.surv) <- 2
age.surv.se <- c("Mean", "SE")
age.surv.se <- as.data.frame(age.surv.se)
names(age.surv.se) <- "labels"
age.surv.se <- "SE"
age.surv.se <- as.data.frame(age.surv.se)
names(age.surv.se) <- "labels"
age.surv$labels <- "Mean"
age.surv$labels <- factor("Mean")
age.groups <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
age.groups
age.groups <- as.data.frame(age.groups)
names(age.groups) <- "Men.Surv"
age.groups <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
age.groups <- as.data.frame(age.groups)
names(age.groups) <- "Men1"
age.surv <- mean(age.groups$Men1, na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men.Surv"
age.groups$Men0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "male"]
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men1"
age.surv <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"], na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men1"
age.surv$Men0 <- mean(titanic$Age[titanic$Survived == 0 & titanic$Sex == "male"], na.rm = T)
age.surv$Women1 <- mean(titanic$Age[titanic$Survived == 1 & titanic$Sex == "female"], na.rm = T)
age.surv$Women0 <- mean(titanic$Age[titanic$Survived == 0 & titanic$Sex == "female"], na.rm = T)
age.surv.se <- "SE"
age.surv.se <- as.data.frame(age.surv.se)
names(age.surv.se) <- "labels"
rm(age.groups)
men1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
titanic <- read.csv("http://maitra.public.iastate.edu/stat579/datasets/titanic.txt")
men1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
men0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "male"]
women1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "female"]
women0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "female"]
men1.mean <- mean(men1, na.rm = T)
age.surv <- mean(men1, na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men1"
age.surv$Men0 <- mean(men0, na.rm = T)
age.surv$Women1 <- mean(women1, na.rm = T)
age.surv$Women0 <- mean(women0, na.rm = T)
age.surv.se <- sd(men1) / sqrt(length(men1))
sd(men1) / sqrt(length(men1))
sum(is.na(men1))
sum(is.na(men0))
men1 <- titanic$Age[!is.na(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"])]
sum(is.na(men1))
men1 <- titanic$Age[!is.na(titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"])]
men1
men1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
men1
men1.na <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male"]
men1 <- men1.na[!is.na(men1.na)]
sum(is.na(men1))
men1.na <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male" & !is.na(titanic$Age)]
men1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male" & !is.na(titanic$Age)]
men0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "male" & !is.na(titanic$Age)]
women1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "female" & !is.na(titanic$Age)]
women0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "female" & !is.na(titanic$Age)]
age.surv <- mean(men1, na.rm = T)
age.surv <- as.data.frame(age.surv)
names(age.surv) <- "Men1"
age.surv$Men0 <- mean(men0, na.rm = T)
age.surv$Women1 <- mean(women1, na.rm = T)
age.surv$Women0 <- mean(women0, na.rm = T)
age.surv.se <- sd(men1) / sqrt(length(men1))
age.surv.se <- as.data.frame(age.surv.se)
names(age.surv.se) <- "Men1"
?var()
age.surv.means <- list(men1 = mean(men1, na.rm = T))
age.surv.means <- list(men1 = mean(men1), men0 = mean(men0))
age.surv.means <- list(men1 = mean(men1), men0 = mean(men0), women1 = mean(women1), women0 = mean(women0))
means <- list(men1 = mean(men1), men0 = mean(men0), women1 = mean(women1), women0 = mean(women0))
rm(age.surv)
rm(age.surv.means)
se <- list(men1 = sd(men1) / sqrt(length(men1)), men0 = sd(men0) / sqrt(length(men0)))
std <- function(x) sd(x) / sqrt(length(x))
std(men1)
groups <- list(men1, men0, women1, women0)
groups
?list()
?pairlist()
groups <- pairlist(men1, men0, women1, women0)
groups
groups <- list(men1, men0, women1, women0, all.names = T)
groups <- list(men1 = men1, men0 = men0, women1 = women1, women0 = women0)
rm(men1, men0, women1, women0)
?lapply()
means <- lapply(groups, mean
)
stderr <- lapply(groups, sd / sqrt(length))
groups[1]
groups[2]
stderr <- apply(groups, i, sd(groups[i]) / sqrt(length(groups[i])))
?apply()
stderr <- lapply(groups, sd/sqrt(length()))
stderr <- lapply(groups, sd/sqrt(length)
)
std <- function(x) sd(x) / sqrt(length(x)) ## function to compute standard error
stderr <- lapply(groups, std)
titanic <- read.csv("http://maitra.public.iastate.edu/stat579/datasets/titanic.txt")
table(titanic$PClass, titanic$Sex)
length(titanic$Sex[titanic$Sex == "male"]) / length(titanic$Sex) ## 64.8% male
source('~/Development/R/stat579/hw2.R', echo=TRUE)
table(titanic$PClass, titanic$Sex, titanic$Survived)
titanic <- read.csv("http://maitra.public.iastate.edu/stat579/datasets/titanic.txt")
(table(titanic$PClass, titanic$Sex, titanic$Survived)[,,2] / (class.gender)) * 100
(table(titanic$PClass, titanic$Sex, titanic$Survived)[,,2] / table(titanic$PClass, titanic$Sex)) * 100
table(titanic$PClass, titanic$Sex, titanic$Survived)
(table(titanic$PClass, titanic$Sex, titanic$Survived)[,,2] / table(titanic$PClass, titanic$Sex)) * 100
table(titanic$PClass, titanic$Sex, titanic$Survived)[,,2] / table(titanic$PClass, titanic$Sex) * 100
table(titanic$Sex, titanic$Survived)
table(titanic$Sex)
table(titanic$Survived)
table(titanic$Sex, titanic$Survived)
table(titanic$Sex, titanic$Survived) ## 66.7% of females survived, 16.7% of males survived
### Gender by Survival Status
table(titanic$Sex, titanic$Survived) ## 66.7% of females survived, 16.7% of males survived
table(titanic$PClass, titanic$Sex) ### Far more men on the ship than women
table(titanic$PClass, titanic$Sex, titanic$Survived)[,,2] / table(titanic$PClass, titanic$Sex) * 100
men <- titanic[titanic$Sex == "male" & !is.na(titanic$Age)]
wommen <- titanic[titanic$Sex == "female" & !is.na(titanic$Age)]
men <- titanic[titanic$Sex == "male"]
men <- titanic[["Age"], ["Survived"]][titanic$Sex == "male" & !is.na(titanic$Age)]
men <- titanic[["Age", "Survived"]][titanic$Sex == "male" & !is.na(titanic$Age)]
titanic[titanic$Sex == "male" & !is.na(titanic$Age)]
men <- titanic$[titanic$Sex == "male" & !is.na(titanic$Age)]
men <- titanic$Age[titanic$Sex == "male" & !is.na(titanic$Age)]
rm(men)
men1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "male" & !is.na(titanic$Age)]
men0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "male" & !is.na(titanic$Age)]
women1 <- titanic$Age[titanic$Survived == 1 & titanic$Sex == "female" & !is.na(titanic$Age)]
women0 <- titanic$Age[titanic$Survived == 0 & titanic$Sex == "female" & !is.na(titanic$Age)]
groups <- list(men1 = men1, men0 = men0, women1 = women1, women0 = women0)
means <- lapply(groups, mean)
rm(men1, men0, women1, women0)
means <- lapply(groups, mean)
std <- function(x) sd(x) / sqrt(length(x)) ## function to compute standard error
stderr <- lapply(groups, std)
means[women1] - means[women0] ## 5.97
means["women1"] - means["women0"] ## 5.97
means[["women1"]] - means[["women0"]] ## 5.97
means[["men1"]] - means[["men0"]] ## -6.37
stderr <- lapply(groups, std)
stderr
read.table("http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt")
read.tsv("http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt")
?read.table
read.delim(http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt, header = TRUE, sep = "\t")
read.delim(http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt, header = TRUE, sep = "\t")
read.delim("http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt", header = TRUE, sep = "\t")
political <-read.delim("http://maitra.public.iastate.edu/stat579/datasets/senate-109.txt", header = TRUE, sep = "\t")
q()
read.csv("~/Dropbox/School/ENGL 302/CandidateSummaryAction.csv")
data <- read.csv("~/Dropbox/School/ENGL 302/CandidateSummaryAction.csv")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
qplot(data = data, geom = "hist", x = "can_par_aff")
qplot(data = data, geom = "histogram", x = "can_par_aff")
qplot(data = data, geom = "histogram", x = can_par_aff)
qplot(data = data, geom = "histogram", x = tot_con, colour = can_par_aff)
qplot(data = data, geom = "histogram", bin = 20000000, x = tot_con, colour = can_par_aff)
data$tot_con <- as.numeric(data$tot_con)
data <- read.csv("~/Dropbox/School/ENGL 302/CandidateSummaryAction.csv")
data <- read.csv("~/Dropbox/School/ENGL 302/CandidateSummaryAction.csv")
qplot(data = data, geom = "histogram", bin = 20000000, x = tot_con, colour = can_par_aff)
qplot(data = data, geom = "histogram", x = tot_con, colour = can_par_aff)
qplot(data = data, geom = "histogram", x = tot_con, color = can_par_aff)
qplot(data = data, geom = "histogram", x = tot_con, fill = can_par_aff)
data$tot_con_m <- data$tot_con / 1000000
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff)
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)")
cPalette <- c("Red", "Blue")
scale_fill_manual(values = cPalette)
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)") + scale_fill_manual(values = cPalette)
cPalette <- c("Blue", "Red")
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)") + scale_fill_manual(values = cPalette)
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)", ylab = "Number of Candidates") + scale_fill_manual(values = cPalette)
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)", ylab = "# of Candidates") + scale_fill_manual(values = cPalette)
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)", ylab = "# of Candidates") + scale_fill_manual(values = cPalette) + opts(legend.title = "Party")
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)", ylab = "# of Candidates") + scale_fill_manual(values = cPalette) + labs(fill = "Party")
data$can_nam[which(max(data$tot_con))]
data$can_nam[which(data$tot_con == max(data$tot_con))]
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con)
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con, stat = "identity")
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con_m, stat = "identity")
?table()
table(data$can_par_aff, data$tot_con_m)
table(data$can_par_aff, sum(data$tot_con_m))
table(data$can_par_aff)
rep.total <- sum(data$tot_con[which(data$can_par_aff == "REP")])
rep.total
rep.total <- sum(data$tot_con[which(data$can_par_aff == "REP")])
dem.total <- sum(data$tot_con[which(data$can_par_aff == "DEM")])
dem.total
table(data$can_par_aff, c(rep.total, dem.total))
tbl <- is.unique(data$can_par_aff)
tbl <- unique(data$can_par_aff)
tbl
tbl$party <- tbl
tbl[0]
tbl[1]
View(tbl)
View(tbl)
tbl$total <- c(rep.total, dem.total)
View(tbl)
table(data$can_par_aff)
tbl$count <- c(15, 4)
tbl$avg <- tbl$total / tbl$count
tbl
table(data$can_par_aff, tbl$avg / 1000000)
table(tbl$party, tbl$avg / 1000000)
table(tbl$party)
tbl
View(tbl)
tbl[1]
tbl[2]
tbl[3]
rm(tbl[1])
qplot(data = data, geom = "histogram", x = tot_con_m, fill = can_par_aff, xlab = "Total Contributions ($ Millions)", ylab = "# of Candidates") + scale_fill_manual(values = cPalette) + labs(fill = "Party")
qplot(data = data, geom = "histogram", x = can_par_aff)
qplot(data = data, geom = "histogram", x = can_par_aff, xlab="Party")
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con_m, stat = "identity")
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con_m, stat = "identity", xlab = "Party", ylab = "Total Contributions ($ Millions)")
qplot(data = data, geom = "histogram", x = can_par_aff, xlab="Party")
qplot(data = data, geom = "histogram", x = can_par_aff, xlab="Party", ylab = "# of Candidates")
qplot(data = data, geom = "bar", x = can_par_aff, y = tot_con_m, stat = "identity", xlab = "Party", ylab = "Total Contributions ($ Millions)")
qplot(data = tbl, geom = "bar", x = party, y = avg, stat = "identity", xlab = "Party", ylab = "Avg Contributions ($ Millions)")
qplot(data = as.data.frame(tbl), geom = "bar", x = party, y = avg, stat = "identity", xlab = "Party", ylab = "Avg Contributions ($ Millions)")
tbl$avg <- tbl$avg / 1000000
qplot(data = as.data.frame(tbl), geom = "bar", x = party, y = avg, stat = "identity", xlab = "Party", ylab = "Avg Contributions ($ Millions)")
setwd("~/Development/R/ura")
library(XML)
input.dir <- "data/ALL39IN"
files.v <- dir(path = input.dir, pattern = "*.xml")
source("code/corpusFunctions1.R")
# Clean documents and import as text array and id array d.f.
getDocumentTokensAsVector <- function(doc.object) {
paras<-getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words<-paste(sapply(paras,xmlValue), collapse=" ")
words.lower<-tolower(words)
words.lower<-gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l<-strsplit(words.lower, "\\s+")
word.v<-unlist(words.l)
return (word.v)
}
getCleanDocument <- function(doc.object) {
paras<-getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words<-paste(sapply(paras,xmlValue), collapse=" ")
words.lower<-tolower(words)
words.lower<-gsub("[^[:alnum:][:space:]']", " ", words.lower)
return (words.lower)
}
topic.m <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, segment=1:nrow(chunk.df), sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m)
}
?paste()
help(segment)
topic.m <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m)
}
View(topic.m)
documents <- as.data.frame(topic.m, stringsAsFactors = F)
colnames(documents) <- c("id", "text")
library(mallet)
library(tm)
writeLines(stopwords(), "data/stoplist1.csv")
mallet.instances <- mallet.import(documents$id, documents$text, "data/stoplist1.csv", FALSE, token.regexp="[\\p{L}']+")
n.topics <- 25
topic.model <- MalletLDA(num.topics = n.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
topic.model$train(1000) ## only took 4 seconds, GPA used 2000
topic.words.m <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = TRUE)
doc.topics.m <- mallet.doc.topics(topic.model, smoothed = TRUE, normalized = TRUE)
docs.length <- rep(chunk.size, length(documents$text))
docs.length <- as.vector(docs.length)
docs.length <- NULL
documents$n.tokens <- NULL
docs.length <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
n.tokens <- length(getDocumentTokensAsVector(doc.object))
topic.m<-rbind(topic.m, segments.m, n.tokens)
}
documents <- as.data.frame(topic.m, stringsAsFactors = F)
colnames(documents) <- c("id", "text", "n.tokens")
topic.m <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m, n.tokens)
}
documents <- as.data.frame(topic.m, stringsAsFactors = F)
colnames(documents) <- c("id", "text")
getNumTokensForDoc <- function(doc.object) {
word.v.l <- getDocumentTokensAsVector(doc.object)
return (length(word.v.l))
}
?rbind()
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m)
docs.length <- rbind(docs.length, getNumTokensForDoc(doc.object))
}
documents$n.tokens <- as.vector(docs.length)
docs.length <- NULL
topic.m <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m)
docs.length <- rbind(docs.length, getNumTokensForDoc(doc.object))
}
documents <- as.data.frame(topic.m, stringsAsFactors = F)
colnames(documents) <- c("id", "text")
length(docs.length)
length(docs.length[1])
as.vector(docs.length)
docs.length <- as.vector(docs.length)
documents$n.tokens <- docs.length
topic.words.m <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = TRUE)
doc.topics.m <- mallet.doc.topics(topic.model, smoothed = TRUE, normalized = TRUE)
docs.length <- as.vector(docs.length)
documents$n.tokens <- docs.length
library(LDAvis)
json <- createJSON(phi = topic.words.m, theta = doc.topics.m, doc.length = documents$n.tokens, vocab = vocabulary, term.frequency = word.freqs$term.freq)
serVis(json)
topic.counts <- rowSums(topic.words)
topic.words <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = FALSE)
topic.counts <- rowSums(topic.words)
topic.proportions <- topic.counts/sum(topic.counts)
library(LDAvis)
phi <- sweep(t(topic.words), MARGIN = 2, FUN = "/", topic.counts)
json <- createJSON(K = n.topics, phi = phi, term.frequency = word.freqs$term.freq,
vocab = vocabulary, topic.proportion = topic.proportions)
doc.topics.m <- mallet.doc.topics(topic.model, smoothed = TRUE, normalized = FALSE)
doc.topics.m <- mallet.doc.topics(topic.model, smoothed = TRUE, normalized = TRUE)
json <- createJSON(theta = doc.topics.m, phi = phi, term.frequency = word.freqs$term.freq, vocab = vocabulary, doc.length = documents$n.tokens)
?sweep()
phi <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = TRUE)
library(LDAvis)
json <- createJSON(theta = doc.topics.m, phi = phi, term.frequency = word.freqs$term.freq, vocab = vocabulary, doc.length = documents$n.tokens)
serVis(json)
setwd("~/Development/R/ura")
library(XML)
input.dir <- "data/ALL39IN"
files.v <- dir(path = input.dir, pattern = "*.xml")
source("code/corpusFunctions1.R")
# Clean documents and import as text array and id array d.f.
getDocumentTokensAsVector <- function(doc.object) {
paras<-getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words<-paste(sapply(paras,xmlValue), collapse=" ")
words.lower<-tolower(words)
words.lower<-gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l<-strsplit(words.lower, "\\s+")
word.v<-unlist(words.l)
return (word.v)
}
getNumTokensForDoc <- function(doc.object) {
word.v.l <- getDocumentTokensAsVector(doc.object)
return (length(word.v.l))
}
getCleanDocument <- function(doc.object) {
paras<-getNodeSet(doc.object,
"/d:TEI/d:text/d:body//d:p",
c(d = "http://www.tei-c.org/ns/1.0"))
words<-paste(sapply(paras,xmlValue), collapse=" ")
words.lower<-tolower(words)
words.lower<-gsub("[^[:alnum:][:space:]']", " ", words.lower)
return (words.lower)
}
docs.length <- NULL
topic.m <- NULL
for (i in 1:length(files.v)) {
doc.object<-xmlTreeParse(file.path(input.dir, files.v[i]),
useInternalNodes=TRUE)
#chunk.df<-makeFlexTextChunks(doc.object, 1, percentage=TRUE)
chunk.df <- getCleanDocument(doc.object)
textname<-gsub("\\..*","", files.v[i])
segments.m<-cbind(paste(textname, i, sep="_"), chunk.df)
topic.m<-rbind(topic.m, segments.m)
docs.length <- rbind(docs.length, getNumTokensForDoc(doc.object))
}
documents <- as.data.frame(topic.m, stringsAsFactors = F)
colnames(documents) <- c("id", "text")
library(mallet)
library(tm)
writeLines(stopwords(), "data/stoplist1.csv")
mallet.instances <- mallet.import(documents$id, documents$text, "data/stoplist1.csv", FALSE, token.regexp="[\\p{L}']+")
n.topics <- 25
topic.model <- MalletLDA(num.topics = n.topics)
topic.model$loadDocuments(mallet.instances)
word.freqs <- mallet.word.freqs(topic.model)
topic.model$train(1000) ## only took 4 seconds, GPA used 2000
topic.words <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = FALSE)
topic.counts <- rowSums(topic.words)
topic.proportions <- topic.counts/sum(topic.counts)
doc.topics.m <- mallet.doc.topics(topic.model, smoothed = TRUE, normalized = TRUE)
vocabulary <- topic.model$getVocabulary()
docs.length <- as.vector(docs.length)
documents$n.tokens <- docs.length
phi <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = TRUE)
# Part 3 - LDAvis
library(LDAvis)
json <- createJSON(theta = doc.topics.m, phi = phi, term.frequency = word.freqs$term.freq, vocab = vocabulary, doc.length = documents$n.tokens)
serVis(json)
